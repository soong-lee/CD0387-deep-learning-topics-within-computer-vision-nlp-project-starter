{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d68c5c52-ef53-4b56-ad14-aaa8538bd691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Image Classification using AWS SageMaker  \n",
       "This project is to classify 133 dog breeds using a pre-trained algorithm. It consists of three main parts; finding optimal hyperparameters, setting up a modeling using the found hyperparameters, and setting up a deployment of the modeling at Endpoints in SageMaker.  \n",
       "\n",
       "### 1. Data set up  \n",
       "- Data was downloaded from a Udacity's AWS s3 bucket.  \n",
       "        - https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip  \n",
       "- This file was unzipped and uploaded to the project folder at s3 bucket.  \n",
       "- The unzipped files contains images of 133 dog breeds with 3 subfolders; train, valid, and test.\n",
       "\n",
       "### 2. Hyperparameter optimization  \n",
       "Three hyperparameters are optimized after setting up a neural network using ResNet50 pre-trained layers with a fully connected top layer added and trained with the dog images.  \n",
       "Hyperparameters optimized: learning rate, batch size, and epoch  \n",
       "```\n",
       "hyperparameter_ranges = {\n",
       "    'learning_rate': ContinuousParameter(0.001, 0.1),\n",
       "    'batch_size': CategoricalParameter([16 ,32, 64, 128, 256])\n",
       "}\n",
       "```\n",
       "The optimal hyperparameters found are: batch_size= 64, learning_rate = 0.03620856736835261.  \n",
       "\n",
       "<img src=\"screen_captures/hyperparam_tuning.jpg\">\n",
       "\n",
       "### 3. Model training\n",
       "\n",
       "Using the hyperparameters found in the previous step, the fully connected layers were further trained.  \n",
       "Script used: train_model.py  \n",
       "    - This file trains a model with the hyperparameters obtained from the previous step. Its content is similar to hpo.py with a slight change with the hyperparameters implemented. The images from train folder were used for training, and the ones from verify folder were used for verification. The images in test folder were never touched.  \n",
       "```\n",
       "model.fc = nn.Sequential(\n",
       "               nn.Linear(num_features, 128),\n",
       "               nn.ReLU(),\n",
       "               nn.Linear(128, num_classes))\n",
       "```\n",
       "\n",
       "```\n",
       "estimator = PyTorch( \n",
       "    entry_point='train_model.py',\n",
       "    role=role,\n",
       "    instance_count=1,\n",
       "    base_job_name='debugger-dog-breeds-job',\n",
       "    instance_type='ml.m5.2xlarge', \n",
       "    framework_version='1.9',\n",
       "    py_version='py38',\n",
       "    hyperparameters=best_hyperparameters,\n",
       "    metric_definitions=metric_definitions,\n",
       "    rules = rules,\n",
       "    profiler_config = profiler_config,\n",
       "    debugger_hook_config = debugger_hook_config\n",
       ")\n",
       "\n",
       "estimator.fit({'train': data_path}, wait=True)\n",
       "```\n",
       "\n",
       "### 4. Debugging and profiling\n",
       "\n",
       "Debugging was performed using CloudWatch logs, and here are some of the cases of debugging.  \n",
       "- There was an error message with exit due to \"cannot assign to null\". From the CloudWatch, I was able to fix the mistake I made in the train_model.py, where \"return model\" statement was missing.\n",
       "- During the model deployment step, I kept getting the predict function invocation timeout. I spent days to diagnose this issue with the help of the CloudWatch logs. The log was saying \"ModuleNotFoundError: No module named 'nvgpu'\". Finally excluding all the possible causes one by one, I figured out that Endpoints has some issue with running GPU, which my mentor shared the same assessment. Per suggestion by the mentor, I installed nvgpu module from the jupyter notebook terminal, the problem did not go away. When I commented out the lines with GPU device at the train_model.py script, it worked.\n",
       "\n",
       "Cross entropy loss vs. training step was plotted to see if train and verification process went reasonably. \n",
       "<img src=\"screen_captures/cross_entropy_loss.jpg\">\n",
       "\n",
       "### 5. Model deploying and predictions\n",
       "\n",
       "inference.py script is used to deploy the model to SageMaker. Since the prediction does not cosume computer power, ml.t2.medium instance type is used.  \n",
       "```\n",
       "pytorch_inference_model = PyTorchModel(\n",
       "            entry_point=\"inference.py\",\n",
       "            role=role,\n",
       "            model_data=model_location,\n",
       "            framework_version=\"1.9\",\n",
       "            py_version=\"py38\",\n",
       "            predictor_cls=ImagePredictor,\n",
       ")\n",
       "\n",
       "predictor = pytorch_inference_model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n",
       "```\n",
       "\n",
       "Endpoint was set up successfully as follows.  \n",
       "<img src=\"screen_captures/endpoint.jpg\">\n",
       "\n",
       "Three predictions are made. It failed to correctly predict Maltese, but succeeded in Basenji and Norwich Terrier.  \n",
       "<p float=\"left\">\n",
       "  <img src=\"screen_captures/maltese_prediction.jpg\" width=\"400\" />\n",
       "  <img src=\"screen_captures/basenji_prediction.jpg\" width=\"200\" /> \n",
       "  <img src=\"screen_captures/norwich_terrier_prediction.jpg\" width=\"500\" />\n",
       "</p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(\"README.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b33914-a369-4067-9229-0eb190038ccb",
   "metadata": {},
   "source": [
    "## Image Classification using AWS SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2aa4d8-412d-433f-b2bd-b50c8822fd12",
   "metadata": {},
   "source": [
    "This project is to classify 133 dog breeds using a pre-trained algorithm. It consists of three main parts; finding optimal hyperparameters, setting up a modeling using the found hyperparameters, and setting up a deployment of the modeling at Endpoints in SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172eada7-4852-4bbf-8da3-4be920bbe8f7",
   "metadata": {},
   "source": [
    "### 1. Data set up "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db72771e-f04f-4d31-afe6-e4e9f2e7dc63",
   "metadata": {},
   "source": [
    "- Data was downloaded from a Udacity's AWS s3 bucket.  \n",
    "        - https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip  \n",
    "- This file was unzipped and uploaded to the project folder at s3 bucket.  \n",
    "- The unzipped files contains images of 133 dog breeds with 3 subfolders; train, valid, and test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8462627a-f8e8-465d-a9cd-74aa9afe603c",
   "metadata": {},
   "source": [
    "### 2. Hyperparameter optimization  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825b37a9-9e15-4f60-ac80-a01676dce6e2",
   "metadata": {},
   "source": [
    "Three hyperparameters are optimized after setting up a neural network using ResNet50 pre-trained layers with a fully connected top layer added and trained with the dog images.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e154d6f1-3e79-4e7a-80d6-25e7f2994eca",
   "metadata": {},
   "source": [
    "Hyperparameters optimized: learning rate, batch size, and epoch.  \n",
    "Script used: hpo.py  \n",
    "    - This file consists of the functions of train, test, data loaders for hyperparameter optimization.\n",
    "```\n",
    "hyperparameter_ranges = {\n",
    "    'learning_rate': ContinuousParameter(0.001, 0.1),\n",
    "    'batch_size': CategoricalParameter([16 ,32, 64, 128]),\n",
    "    'epochs': IntegerParameter(2, 6)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aebef6-3ba5-4905-8513-0ada0ecaf863",
   "metadata": {},
   "source": [
    "The optimal hyperparameters found are: batch_size = 128, learning_rate = 0.0016442868842593914, epochs = 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1135d08-27b9-4e2e-825d-186e4065ad60",
   "metadata": {},
   "source": [
    "<img src=\"screen_captures/hyperparam_tuning.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef55427e-67b9-4e04-93bc-3d6e7b187f5e",
   "metadata": {},
   "source": [
    "### 3. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25cff23-420c-4c92-82e3-b534d9a04dae",
   "metadata": {},
   "source": [
    "Using the hyperparameters found in the previous step, the fully connected layers were further trained.  \n",
    "Script used: train_model.py  \n",
    "    - This file trains a model with the hyperparameters obtained from the previous step. Its content is similar to hpo.py with a slight change with the hyperparameters implemented. The images from train folder were used for training, and the ones from verify folder were used for verification. The images in test folder were never touched.  \n",
    "```\n",
    "model.fc = nn.Sequential(\n",
    "               nn.Linear(num_features, 128),\n",
    "               nn.ReLU(),\n",
    "               nn.Linear(128, num_classes))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8238af98-41fa-4cf5-980d-74cde622dd23",
   "metadata": {},
   "source": [
    "```\n",
    "estimator = PyTorch( \n",
    "    entry_point='train_model.py',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    base_job_name='debugger-dog-breeds-job',\n",
    "    instance_type='ml.m5.2xlarge', \n",
    "    framework_version='1.9',\n",
    "    py_version='py38',\n",
    "    hyperparameters=best_hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    "    rules = rules,\n",
    "    profiler_config = profiler_config,\n",
    "    debugger_hook_config = debugger_hook_config\n",
    ")\n",
    "\n",
    "estimator.fit({'train': data_path}, wait=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02820d90-e375-4ec0-8124-07f479fc8e72",
   "metadata": {},
   "source": [
    "### 4. Debugging and profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5724ee-357f-4c81-b2cb-fbc432fe1334",
   "metadata": {},
   "source": [
    "Debugging was performed using CloudWatch logs, and here are some of the cases of debugging.  \n",
    "- There was an error message with exit due to \"cannot assign to null\". From the CloudWatch, I was able to fix the mistake I made in the train_model.py, where \"return model\" statement was missing.\n",
    "- During the model deployment step, I kept getting the predict function invocation timeout. I spent days to diagnose this issue with the help of the CloudWatch logs. The log was saying \"ModuleNotFoundError: No module named 'nvgpu'\". Finally excluding all the possible causes one by one, I figured out that Endpoints has some issue with running GPU, which my mentor shared the same assessment. Per suggestion by the mentor, I installed nvgpu module from the jupyter notebook terminal, the problem did not go away. When I commented out the lines with GPU device at the train_model.py script, it worked.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a1557-3ee1-40e9-8894-a68b2e2a0681",
   "metadata": {},
   "source": [
    "Cross entropy loss vs. training step was plotted to see if train and verification process went reasonably. \n",
    "<img src=\"screen_captures/cross_entropy_loss.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f85977-e789-4eae-8059-d2f9969fb1d7",
   "metadata": {},
   "source": [
    "### 5. Model deploying and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6412cea-b000-46dd-9454-f561c56b34d2",
   "metadata": {},
   "source": [
    "inference.py script is used to deploy the model to SageMaker. Since the prediction does not cosume computer power, ml.t2.medium instance type is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea4db4-a247-4488-9787-339e886362a5",
   "metadata": {},
   "source": [
    "```\n",
    "pytorch_inference_model = PyTorchModel(\n",
    "            entry_point=\"inference.py\",\n",
    "            role=role,\n",
    "            model_data=model_location,\n",
    "            framework_version=\"1.9\",\n",
    "            py_version=\"py38\",\n",
    "            predictor_cls=ImagePredictor,\n",
    ")\n",
    "\n",
    "predictor = pytorch_inference_model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68fd7d2-9902-4251-ac37-332b6b871a6b",
   "metadata": {},
   "source": [
    "Endpoint was set up successfully as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45610830-5fde-4288-b798-6ec459911410",
   "metadata": {},
   "source": [
    "<img src=\"screen_captures/endpoint.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce4e18d-a6bb-4157-a66e-4e635b232404",
   "metadata": {},
   "source": [
    "Three predictions are made. It failed to correctly predict Maltese, but succeeded in Basenji and Norwich Terrier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9289ae04-35a1-4258-8c54-7d12c0faa3f3",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "  <img src=\"screen_captures/maltese_prediction.jpg\" width=\"400\" />\n",
    "  <img src=\"screen_captures/basenji_prediction.jpg\" width=\"200\" /> \n",
    "  <img src=\"screen_captures/norwich_terrier_prediction.jpg\" width=\"500\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e5f4da-90bf-4967-90e6-1a48acf66b58",
   "metadata": {},
   "source": [
    "### Insights from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76934203-ee3c-4d23-9a41-58885d94c676",
   "metadata": {},
   "source": [
    "This was the first time I developed a model using a pre-trained algorithm. The result is quite impressive, and most of all it worked. Going through setting up the Endpoint in SageMaker and finally running the predictions was an awesome experience. Along the way there are a few points in SageMaker that I could not quite understand, but that will be my next challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1cc753-d919-4294-b48c-7d62ba80d546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d361a-fc9e-481e-b1a6-3eb488800142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f599f-0218-4aff-ac5f-f93dec1012ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
